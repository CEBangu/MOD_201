{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network output is defined as (pg 547):\n",
    "\n",
    "$$z(t) = \\mathbf{w}^T \\mathbf{r}(t)$$\n",
    "\n",
    "network error function (pg 547):\n",
    "\n",
    "$$e_-(t) = \\mathbf{w}^T(t - \\Delta t)\\mathbf{r}(t) - f(t)$$\n",
    "\n",
    "nb! this is the error right before the weights are updated at time t. The idea is to update the weights from $\\mathbf{w}(t-\\Delta t) \\text{ to } \\mathbf{w}(t)$ in such a way as to reduce the magnitude of $e_-(t)$\n",
    "\n",
    "So, immediately after the weight update, the netwrok output, **z**, is $\\mathbf{w}^T(t)\\mathbf{r}(t)$, if the weights are updated fast enough.\n",
    "\n",
    "So, the error __after__ the weight update is $$ e_+(t) = \\mathbf{w}^T(t) \\mathbf{r}(t) - f(t) $$\n",
    "\n",
    "\n",
    "THE goal, then, is to reduce errors in the network by making $$|e_+(t)| < |e_-(t)|$$ and converging to a solution so that the weight vecotr no longer needs to be updated (pg. 548)\n",
    "\n",
    "this means that $$\\frac{e_+(t)}{e_-(t)} \\rightarrow 1$$ by the end of training (pg 548)\n",
    "\n",
    "accomplished by steady but small reduction in $e_-$ and $e_+$ over time, except for the first update which is large (pg 548). So you also want to have $\\Delta t$ be pretty short to accomplish this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Least Squares\n",
    "\n",
    "(Why?)\n",
    "\n",
    "RLS: $$\\mathbf{w}(t) = \\mathbf{w}(t - \\Delta t) - e_-(t) \\mathbf{P}(t) \\mathbf{r}(t)$$\n",
    "\n",
    "where **P** is an NxN matrix that is updated at the same time as the weights according to the rule: \n",
    "$$\\mathbf{P}(t) = \\mathbf{P}(t - \\Delta t) - \\frac{\\mathbf{P}(t - \\Delta t) \\mathbf{r}(t) \\mathbf{r}^T \\mathbf{P}(t - \\Delta t)}{1 + \\mathbf{r}^T(t)\\mathbf{P}(t - \\Delta t) \\mathbf{r}(t)}$$\n",
    "\n",
    "Where $$\\mathbf{P}(0) = \\frac{\\mathbf{I}}{\\alpha}$$\n",
    "Where **I** is the identity matrix, and $\\alpha$ is a constant (pg. 548)\n",
    "\n",
    "Where **P** acts a a multidimensional learning rate for the 'delta' type learning rule. So, for this algorithm, **P** is a \"Running estimate  of the inverse of teh correlation matrix of the network rates **r** plus a regularization term: $$\\mathbf{P} = \\left( \\sum_t \\mathbf{r}(t)\\mathbf{r}^T(t) + \n",
    "\\alpha \\mathbf{I} \\right)$$\n",
    "\n",
    "For subsequent timesteps, the equation above imply that: $$e_+(t) = e_-(t)(1 - \\mathbf{r}^T (t) \\mathbf{P}(t)\\mathbf{r}(t))$$\n",
    "(pg 548) so the quantity $\\mathbf{r}^T\\mathbf{P}\\mathbf{r}$ varies from around 1 to assymptotically approaching 0 over the course of training, and is always positive\n",
    "\n",
    "$\\alpha$ acts as the learning rate, and should be adjusted depending on the target function. (pg548). small values for $\\alpha$ result in fast learning but sometimes can become unstable. But, if $\\alpha$ is too big, FORCE might not be able to keep the output close to the target for long enough. use between 1 and 100 (pg 548)\n",
    "\n",
    "Learning generally takes about $1000 \\tau$ where $\\tau$ is the basic time constant of the network, which they set to around 10ms. So basically it takes around 10 seconds of simulated time for teh network to converge (549)\n",
    "\n",
    "\"To encompass all the models, we write the newtwork equations for the generator network as: \n",
    "$$\\tau \\frac{dx_i}{dt} = -x_i + g_{GG} \\sum_{j = 1}^{N_G}J_{ij}^{GG} r_j + g_{G_z} J_{i} ^{G_z} z + g_{GF} \\sum_{a = 1}^{N_F} J_{ia}^{GF} s_a + \\sum_{\\mu = 1}^{N_I} J_{i \\mu}^{GI} I_\\mu$$\n",
    "\n",
    "for $i = 1, 2, .. <1,2,..>, N_G$ with firing rates $r_i = \\tanh(x_i$) (pg 556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For figure 2 and 3, the parameters are:\n",
    "\n",
    "$$N_G = 1000\\\\\n",
    "p_{GG} = 0.1\\\\\n",
    "p_Z = 1\\\\\n",
    "g_{Gz} = 1\\\\\n",
    "g_{GF} = 0\\\\\n",
    "\\alpha = 1.0\\\\\n",
    "N_I = 0\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
